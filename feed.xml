<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://varungohil.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://varungohil.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-06T19:55:11+00:00</updated><id>https://varungohil.github.io/feed.xml</id><title type="html">blank</title><subtitle>Varun&apos;s piece of the web </subtitle><entry><title type="html">Surprising Power Dependence between CPU Sockets</title><link href="https://varungohil.github.io/blog/2025/powerexp/" rel="alternate" type="text/html" title="Surprising Power Dependence between CPU Sockets"/><published>2025-04-20T04:00:00+00:00</published><updated>2025-04-20T04:00:00+00:00</updated><id>https://varungohil.github.io/blog/2025/powerexp</id><content type="html" xml:base="https://varungohil.github.io/blog/2025/powerexp/"><![CDATA[<p>While digging into Linux power management over the past few weeks, I stumbled upon an interesting phenomenon, on multi-socket machines, the core frequency of one socket affects the power consumption of other sockets, even when controlling for their frequency, utilization and workload!</p> <p>While some power interaction between sockets is expected due to heat transfer and thermal effects, what makes this discovery particularly interesting is the magnitude of power dependence between sockets and the surprising mechanism behind it. This blog post documents my investigation and explains how Intel’s power management policy creates this hidden power relationship.</p> <h4 id="experimental-setup"><strong>Experimental Setup</strong></h4> <p>To study this phenomenon, I designed a minimal setup as shown in Figure 1. Using c220g5 (Intel Xeon Silver 4114) machines on CloudLab with 2 sockets, I created the following configuration:</p> <ul> <li><strong>Socket 1 (Active socket)</strong>: Running multiple busy loops (programs with infinite for loops) to control utilization. Each busy loop fully utilizes one core. The frequency of all cores on Socket 1 was fixed at 0.8 GHz, the lowest possible frequency on the machine.</li> <li><strong>Socket 0 (Passive socket)</strong>: Running an energy measurement script pinned to a single core, regularly polling the energy counter using the <code class="language-plaintext highlighter-rouge">powercap-info</code> command.</li> </ul> <p>By measuring energy consumption over short intervals, I could calculate power consumption by dividing energy by time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/powerexp/setup-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/powerexp/setup-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/powerexp/setup-1400.webp"/> <img src="/assets/img/powerexp/setup.png" class="img-fluid rounded z-depth-1" width="500" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Experimental Setup using c220g5 nodes on CloudLab </div> <h4 id="the-mysterious-socket-power-dependence"><strong>The Mysterious Socket Power Dependence</strong></h4> <p>For my experiment, I varied the frequency of Socket 0 while measuring power consumed by both sockets. Since Socket 1’s core frequency was fixed throughout the experiment, I expected that changing Socket 0’s frequency would have little to no impact on Socket 1’s power consumption. However, my results showed something completely different!</p> <p>Figure 2 shows the power consumed by both sockets when Socket 1 is at 40% utilization as I vary Socket 0’s frequency. When I increased Socket 0’s frequency from 0.8 GHz to 2.0 GHz, Socket 1’s power consumption remained stable as expected. However, when I increased Socket 0 to 2.2 GHz, Socket 1’s power consumption suddenly jumped (brown line)!</p> <p>This pattern appeared consistently across different utilization levels and even when swapping which socket was active. Two aspects of this result were particularly surprising:</p> <ul> <li>The power dependency was triggered only when Socket 0 ran at 2.2 GHz (the highest frequency of the machine)</li> <li>The difference in Socket 1’s power consumption between 2.0 GHz and 2.2 GHz settings was approximately 5W — a significant amount when Socket 1’s power consumption is around 30W.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/powerexp/uncorevary-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/powerexp/uncorevary-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/powerexp/uncorevary-1400.webp"/> <img src="/assets/img/powerexp/uncorevary.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Power consumption of both sockets when varying Socket 0's frequency while Socket 1 runs at 40% utilization </div> <h4 id="the-culprit-uncore-frequency"><strong>The Culprit: Uncore frequency</strong></h4> <p>To identify the cause of this socket power dependence, I collected various microarchitectural metrics. One metric stood out with a perfect correlation to the observed power behavior: uncore frequency.</p> <p>The “uncore” refers to components of the CPU that are not part of the cores themselves—including memory controllers, last-level cache, and interconnects between cores. These components have their own clock frequencies that can be adjusted separately from core frequencies.</p> <p>Figure 3 shows the uncore frequency when varying Socket 0’s core frequency. The uncore frequency remained at its minimum value (1.2 GHz) when Socket 0’s core frequency was between 0.8 GHz and 2.0 GHz. However, when the core frequency reached 2.2 GHz, the uncore frequency suddenly jumped to its maximum value of 2.0 GHz.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/powerexp/uncore-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/powerexp/uncore-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/powerexp/uncore-1400.webp"/> <img src="/assets/img/powerexp/uncore.png" class="img-fluid rounded z-depth-1" width="500" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Uncore frequency behavior when varying Socket 0's core frequency </div> <p>To verify that uncore frequency was indeed responsible for the power dependency, I conducted another experiment where I fixed the uncore frequency at 1.2 GHz while varying Socket 0’s core frequency. Figure 4 shows the results—Socket 1’s power consumption no longer depended on Socket 0’s core frequency when the uncore frequency was fixed.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/powerexp/uncorefix-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/powerexp/uncorefix-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/powerexp/uncorefix-1400.webp"/> <img src="/assets/img/powerexp/uncorefix.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Power consumption of both sockets when varying Socket 0's frequency with uncore frequency fixed at 1.2 GHz </div> <p><strong>Why does this happen?</strong></p> <p>This behavior is explained by Intel’s Uncore Frequency Scaling policy. According to research by <a href="https://yananguo.com/files/micro23.pdf">Guo et al.</a>:</p> <blockquote> <p>Uncore frequency is dynamically adjusted only when all the active cores are running at a frequency lower than the base frequency. When at least one core is running at a higher frequency, the uncore consistently stays at the maximum frequency”</p> </blockquote> <p>For my test machine, the base frequency is 2.2 GHz. Therefore, when I set Socket 0’s core frequency to 2.2 GHz, the uncore frequency automatically increased to its maximum value, causing increased power consumption on the adjacent socket as well.</p> <p><strong>What’s the rationale behind this design?</strong></p> <p>While this explains the mechanism behind the socket power dependency, it raises the question of why Intel designed their uncore frequency scaling this way.</p> <p>The best explanation comes from a paper by <a href="https://lca.ece.utexas.edu/pubs/BircherICS2008.pdf">Bircher et al.</a>, which suggests that when a core runs at high frequency, it can be stalled by delays in cache snooping requests if the uncore frequency is too low. Running the uncore at maximum frequency when any core reaches maximum frequency helps prevent these bottlenecks.</p> <p>However, this optimization only makes sense for memory-intensive applications. In my experiment with CPU-bound busy loops requiring minimal memory access, this behavior seems unnecessary yet still impacts system-wide power consumption.</p> <p>If you have alternative explanations for this behavior or have observed similar phenomena, I’d love to hear from you!</p> <h4 id="key-takeaways"><strong>Key Takeaways</strong></h4> <p>This investigation has revealed a significant and often overlooked power dependency in multi-socket systems, where the core frequency of one socket can unexpectedly impact the power consumption of adjacent sockets due to scaling of uncore frequency.</p> <p>This finding has important implications for power management in multi-socket systems:</p> <ul> <li>Power Modeling: Accurate power models must consider the cross-socket effects of uncore frequency scaling to provide reliable power estimates for diverse workloads.</li> <li>Workload Placement: Careful consideration should be given to the placement of high-frequency workloads. Running applications that heavily utilize the maximum core frequency on one socket could inadvertently increase the power draw of other sockets, potentially impacting overall system efficiency and thermal management.</li> </ul>]]></content><author><name></name></author><category term="technical"/><category term="computer-systems"/><category term="power-management"/><summary type="html"><![CDATA[Understanding the hidden impacts of uncore frequency scaling]]></summary></entry><entry><title type="html">Microsecond-Level Time Synchronization on CloudLab</title><link href="https://varungohil.github.io/blog/2025/timesync-cloudlab/" rel="alternate" type="text/html" title="Microsecond-Level Time Synchronization on CloudLab"/><published>2025-03-16T01:20:00+00:00</published><updated>2025-03-16T01:20:00+00:00</updated><id>https://varungohil.github.io/blog/2025/timesync-cloudlab</id><content type="html" xml:base="https://varungohil.github.io/blog/2025/timesync-cloudlab/"><![CDATA[<p>Ever tried to run distributed experiments only to have your Jaeger traces and results ruined by out-of-sync time? Yeah, been there! After banging my head against this problem for a while, I finally found a simple solution to get microsecond-level time synchronization between <a href="https://cloudlab.us/">CloudLab</a> machines. Let me walk you through it!</p> <h4 id="the-problem-internet-ntp-is-too-slow"><strong>The Problem: Internet NTP Is Too Slow</strong></h4> <p>When you setup up <a href="https://cloudlab.us/">CloudLab</a> machines, they’re configured to sync their time with Internet NTP servers. Sounds great, but there’s a problem - it leads to significant millisecond-level time differences between machines that are synchronizing with that internet NTP server.</p> <p>Let’s start by measuring time difference on a 2-machine setup. I’ll be using <strong><em>chrony</em></strong> - an NTP tool; so first, let’s install chrony on both machines:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt-get update
sudo apt-get install chrony
</code></pre></div></div> <p>Now, to measure the time difference, run the following command on machines:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>chronyc tracking
</code></pre></div></div> <p>Images below show the output of the command on both machines:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/time-sync/initial-skew-measurement-0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/time-sync/initial-skew-measurement-0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/time-sync/initial-skew-measurement-0-1400.webp"/> <img src="/assets/img/time-sync/initial-skew-measurement-0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/time-sync/initial-skew-measurement-1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/time-sync/initial-skew-measurement-1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/time-sync/initial-skew-measurement-1-1400.webp"/> <img src="/assets/img/time-sync/initial-skew-measurement-1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There are a lot of numbers in the images above, but the most important ones for us are <em>Last Offset</em> and <em>RMSE Offset</em>. Last offset reports the last measured time difference between our machine and the reference machine. RMSE offset reports the root mean squared time difference between our machine and the reference machine over a period of time.</p> <p>For node-0, the last offset is 20.3 μs and RMSE offset is 490 μs. For node-1, both the last offset and RMSE offset values are 2.36 ms! This means the time difference between both machines is also in the order of a millisecond. While this might sound small, it is a huge problem when dealing with microservice application where each service takes only 100s of microseconds. A time difference of a milliseond can lead to useless and garbage Jaeger traces which do not facilitate debugging or performance analysis.</p> <p>So, let’s see what is this authoritaive time source server that our machine is talking to. Take a peek at in <code class="language-plaintext highlighter-rouge">/etc/chrony/chrony.conf</code>, you’ll see some lines like these:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pool ntp.ubuntu.com        iburst maxsources 4
pool 0.ubuntu.pool.ntp.org iburst maxsources 1
pool 1.ubuntu.pool.ntp.org iburst maxsources 1
pool 2.ubuntu.pool.ntp.org iburst maxsources 2
</code></pre></div></div> <p>These lines tell chrony to synchronize your machine’s time with time from one of the following sources: <code class="language-plaintext highlighter-rouge">ntp.ubuntu.com </code>, <code class="language-plaintext highlighter-rouge">0.ubuntu.pool.ntp.org</code>, <code class="language-plaintext highlighter-rouge">1.ubuntu.pool.ntp.org</code> and <code class="language-plaintext highlighter-rouge">2.ubuntu.pool.ntp.org</code>.</p> <p><code class="language-plaintext highlighter-rouge">ntp.ubuntu.com</code> is the official Ubuntu NTP server while <code class="language-plaintext highlighter-rouge">*.ubuntu.pool.ntp.org</code> are servers part of the <a href="https://www.ntppool.org/en/">NTP Pool Project</a>. More importantly, all these authoritative time sources are accessible on the internet and are not locally located with your cluster. This results in longer communication times between your machines and internet NTP server, which can degrade time synchronization. You can also see this in the images above. <em>Root Delay</em> measures the total round-trip network delay between your machine and the NTP server. The root delay on node-0 is 46ms while on node-1 it is 24.5ms. Having millisecond level root delays makes it harder to achieve microsecond level time synchronization. So what do we do?</p> <h4 id="the-solution-local-time-server"><strong>The Solution: Local Time Server</strong></h4> <p>Here’s the simple and obvious trick: instead of having all your machines sync with Internet NTP servers, designate one machine as the local time server and have the others sync with it. This avoids the internet access latency, lowers your root delay and dramatically improves time synchronization.</p> <p>Here, I’m going to designate node-0 as my local time server and synchronize node-1’s time with my local node-0 time server. Here’s how to set it up:</p> <p>On the machine you want to use as your time server (I chose node-0), modify its <code class="language-plaintext highlighter-rouge">/etc/chrony/chrony.conf</code> like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#pool ntp.ubuntu.com        iburst maxsources 4
#pool 0.ubuntu.pool.ntp.org iburst maxsources 1
#pool 1.ubuntu.pool.ntp.org iburst maxsources 1
#pool 2.ubuntu.pool.ntp.org iburst maxsources 2

local stratum 10

allow 10.10.1.2
</code></pre></div></div> <p>Here I’ve commented out lines asking chrony to sync time with internet NTP servers. The <code class="language-plaintext highlighter-rouge">local stratum</code> line makes node-0 act as a time server now. The number following it can be any number greater than 0. The <code class="language-plaintext highlighter-rouge">allow &lt;ip&gt;</code> line allows the machine with the given ip to use this server as it’s time server. In my setup 10.10.1.2 was the ip of node-1 so this line allows node-1 to use node-0 as a time server.</p> <p>On the machines who should be using this new local time server (node-1 in my case), modify the <code class="language-plaintext highlighter-rouge">/etc/chrony/chrony.conf</code> like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#pool ntp.ubuntu.com        iburst maxsources 4
#pool 0.ubuntu.pool.ntp.org iburst maxsources 1
#pool 1.ubuntu.pool.ntp.org iburst maxsources 1
#pool 2.ubuntu.pool.ntp.org iburst maxsources 2

# default minpoll 6 maxpoll 10
server 10.10.1.1 iburst prefer minpoll 4 maxpoll 6
</code></pre></div></div> <p>Here, again I’ve commented out lines asking chrony to sync time with internet NTP servers. The <code class="language-plaintext highlighter-rouge">server &lt;ip&gt; iburst prefer</code> line specifies the ip of the time server your machine should use. The <code class="language-plaintext highlighter-rouge">minpoll 4 maxpoll 6</code> part of the line is not necessary. It sets the minimum polling period to $2^{4} = 16$ s and maximum polling period to $2^{6} = 64$ s.</p> <p>After modifying the files, to apply your changes, you’ll need to restart chrony:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl restart chrony
</code></pre></div></div> <p>Having made these changes, I got dramatically better time synchronization. The images below show output of <code class="language-plaintext highlighter-rouge">chronyc tracking</code> after the modifications:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/time-sync/final-skew-measurement-0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/time-sync/final-skew-measurement-0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/time-sync/final-skew-measurement-0-1400.webp"/> <img src="/assets/img/time-sync/final-skew-measurement-0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/time-sync/final-skew-measurement-1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/time-sync/final-skew-measurement-1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/time-sync/final-skew-measurement-1-1400.webp"/> <img src="/assets/img/time-sync/final-skew-measurement-1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, for node-0, you’ll see that all metrics are 0, since it is now our authoritative time server. On node-1, you see that root delay decreased significantly - from 24.5ms to 70μs! This is because now the time server is locally situated. This decrease in root delay also helps time synchronization. The RMSE offset from 2.4ms to 2.5μs! This new time difference which is in order of a few microseconds is sufficient for getting reliable Jaeger traces for microservice applications!</p> <p>Hence, setting up one machine as a local time server, we dramatically improved clock synchronization between our <a href="https://cloudlab.us/">CloudLab</a> machines. This approach might not be the best approach is you want precision in absolute time reporting, however, it is perfect for most distributed systems experiments where time difference between machines is critical.</p> <p>The best part? It’s super easy to set up. Just a few config file tweaks, and you’re on your way to microsecond-level time synchronization!</p> <p>Here are the full <code class="language-plaintext highlighter-rouge">/etc/chrony/chrony.conf</code> config files if you are interested:</p> <ul> <li><a href="/assets/img/time-sync/initial-chrony-conf-0.txt">Initial config file for node-0</a></li> <li><a href="/assets/img/time-sync/initial-chrony-conf-1.txt">Initial config file for node-1</a></li> <li><a href="/assets/img/time-sync/final-chrony-conf-0.txt">Modified config file for node-0</a></li> <li><a href="/assets/img/time-sync/final-chrony-conf-1.txt">Modified config file for node-1</a></li> </ul>]]></content><author><name></name></author><category term="technical"/><category term="computer-systems"/><summary type="html"><![CDATA[for reliable jaeger tracing]]></summary></entry><entry><title type="html">Interview with Quill - IIT Gandhinagar</title><link href="https://varungohil.github.io/blog/2025/quill-iitgn/" rel="alternate" type="text/html" title="Interview with Quill - IIT Gandhinagar"/><published>2025-02-01T05:53:00+00:00</published><updated>2025-02-01T05:53:00+00:00</updated><id>https://varungohil.github.io/blog/2025/quill-iitgn</id><content type="html" xml:base="https://varungohil.github.io/blog/2025/quill-iitgn/"><![CDATA[<p>Thanks to <a href="https://in.linkedin.com/in/aarsh-wankar">Aarsh Wankar</a> for conducting this interveiw.</p> <h4 id="could-you-share-some-formative-experiences-from-your-time-at-iitgn-that-inspired-or-influenced-your-decision-to-pursue-a-phd-at-mit"><strong>Could you share some formative experiences from your time at IITGN that inspired or influenced your decision to pursue a PhD at MIT?</strong></h4> <p>Reflecting back, it’s difficult to point to singular moments that led me to pursue a PhD. Rather, it was a process of discovering the intersection of “things I enjoyed doing” and “things I was good at.”</p> <p>Fortunately, I discovered research lay in that intersection for me quite early on and this was reinforced by several experiences. SRIP gave me my first taste of research, my internship at University of Utah exposed me to international research standards, and working as a TA with Prof. Nipun Batra helped me discover that I enjoy teaching - all key aspects of academic life. But equally important were the experiences that showed me what wasn’t the right fit. I tried my hand at entrepreneurship, took leadership roles in student organizations (Amalthea and Election Commission), and explored conventional software development. While valuable experiences, I either didn’t excel at these activities or simply didn’t enjoy them as much as research.</p> <p>I think the most significant impact was made by having regular interactions with my excellent peer group (shoutout to Shreyas, Ritik, Sagar, Kshitij, Nisarg and Akhil). Not all of them wanted to end up as researchers, however, they all have qualities I admire which I have tried to imbibe.</p> <p>Looking back, it was really this process of trial and error - trying different things, failing at some, succeeding at others - that helped me realize that pursuing a PhD was the right choice for me.</p> <h4 id="how-did-you-first-get-interested-in-computer-architecture-what-role-did-your-srip-experience-on-mobile-computer-architecture-play-in-shaping-your-career-path"><strong>How did you first get interested in computer architecture? What role did your SRIP experience on mobile computer architecture play in shaping your career path?</strong></h4> <p>My first interaction with computer architecture was during the Computer Architecture course taught by Prof. Manu Awasthi. What got me hooked was how tangible it felt. Unlike prior courses which talked about abstract concepts like data structures, I could disassemble my laptop and see the actual processor chip and DRAM DIMMS we were studying in the course. Having enjoyed the course, I interned with Prof. Awasthi during SRIP which first introduced me to research.</p> <p>The SRIP experience had two significant impacts on my career path. First, it helped me discover that research was something I not only enjoyed but showed aptitude for. This realization made me start considering research as a potential long-term career rather than just a summer activity. Second, our work was accepted as a poster at a prestigious conference, which provided external validation of my research abilities. This early success helped build my confidence and strengthened my profile, allowing me to pursue more research opportunities later on.</p> <h4 id="reflecting-on-your-undergraduate-projects-at-iitgn-such-as-approximate-computing-neural-network-compression-and-hybrid-memory-systems-how-did-these-prepare-you-for-your-current-research"><strong>Reflecting on your undergraduate projects at IITGN, such as approximate computing, neural network compression, and hybrid memory systems, how did these prepare you for your current research?</strong></h4> <p>Looking back at my undergrad research projects, what’s funny is that I don’t actually use much of that technical knowledge in my current work. But working on those projects was not a waste of time - they taught me how to approach complex research problems. I worked on all sorts of things, from approximate computing to neural networks, and not everything worked out perfectly. But that’s exactly what helped me build confidence to take on high-risk problems during my PhD. Further, I collaborated with a large set of people across the projects I pursued. Collaborating broadly exposed me to a diverse array of working styles, and I have picked up good working habits from my collaborators which continue to serve me well till date.</p> <h4 id="could-you-explain-your-current-research-in-simple-terms-for-readers-who-may-not-be-familiar-with-the-field"><strong>Could you explain your current research in simple terms for readers who may not be familiar with the field?</strong></h4> <p>My work has two main research threads, both focused on making large datacenters run better. But first, let me explain what a datacenter is - imagine a massive warehouse filled with hundreds of thousands of computers (servers) all connected together. These datacenters power the internet services we use every day, from email to social media to cloud storage. So every time you search on Google or buy something on Amazon, the computation is run in one such datacenter.</p> <p>The first challenge I’m tackling is power management. With the rise of AI models like ChatGPT and many new applications, datacenters are consuming more electricity than they were designed to handle. This isn’t just a cost problem - it’s also a climate issue, especially when that power comes from non-renewable sources. My research focuses on finding ways to reduce this power consumption while keeping services running smoothly.</p> <p>The second thread of my research deals with AI interpretability. When you have hundreds of thousands of servers, it’s cumbersome to manage them manually. So companies use AI models to help make decisions about things like which programs should run on which servers. However, when these AI models make mistakes, it’s really hard to figure out why they made those decisions and how to fix them. Currently, the main solution is to retrain the entire model, which is expensive and time-consuming. I’m working on something called “mechanistic interpretability” - essentially trying to reverse-engineer these AI models and understand their decision-making process, so we can make targeted fixes when they make mistakes.</p> <h4 id="machine-learning-is-often-seen-as-a-soft-computing-domain-while-computer-systems-are-more-rule-based-and-deterministic-how-do-you-integrate-ml-models-into-computer-systems-tasks-while-ensuring-the-overall-system-remains-reliable"><strong>Machine learning is often seen as a “soft” computing domain, while computer systems are more rule-based and deterministic. How do you integrate ML models into computer systems tasks while ensuring the overall system remains reliable?</strong></h4> <p>Great question! Maintaining overall system reliability for systems that use ML models is still very much an open challenge. Currently, at a high level, researchers are adopting two approaches: interpretability and predictability.</p> <p>Interpretability focuses on answering the question, “Why does the model make a particular decision?” This is a vital step towards building trust and understanding in ML-based systems. One fascinating development here is mechanistic interpretability. For example, researchers have isolated specific neurons in large language models, like Claude, that represent concepts such as the Golden Gate Bridge. This means that if you wanted the model to forget about the Golden Gate Bridge, you could potentially modify or remove just that neuron, rather than retraining the entire model. While this is a promising approach, the growing complexity and size of models make this more challenging and most researchers view this approach as a long-term bet.</p> <p>Predictability, on the other hand, takes a more pragmatic/relaxed approach by addressing “When does the model make a wrong decision?” This can be achieved with uncertainty estimators or out-of-distribution detectors, which are tools designed to flag situations where the model’s predictions are likely unreliable. By proactively identifying these cases, systems can fall back on well-tested heuristics or deterministic methods, ensuring overall reliability even in the face of potential ML errors.</p> <h4 id="after-your-srip-internship-you-spent-the-summer-of-your-third-year-interning-at-the-university-of-utah-under-prof-rajeev-balasubramonian-what-advice-would-you-give-to-iitgn-students-aspiring-to-intern-at-universities-abroad"><strong>After your SRIP internship, you spent the summer of your third year interning at the University of Utah under Prof. Rajeev Balasubramonian. What advice would you give to IITGN students aspiring to intern at universities abroad?</strong></h4> <p>Back when I was applying, most foreign internships were research-oriented. Something that helped me was getting my hands dirty with projects before applying. Try making significant contributions to some research or technical project. Having said that what is important is not just the work you do, but to really understand why you are doing it. Whether it’s research or other technical projects, try to pick things you genuinely care about - it makes such a difference when you’re discussing them in applications or interviews.</p> <p>One thing I can’t ignore from my experience - and I know this might not be what everyone wants to hear - is that CPI really did matter. The internship priority was pretty much tied to academic performance. That said, please don’t let this discourage you if your CPI isn’t exactly where you’d like it to be.</p> <p>For internship programs like DAAD and MITACS you need a well-tailored CV with solid research experience and good academic achievements. Though I was rejected from these programs myself, I know many who were not, so don’t let my experience discourage you from applying.</p> <h4 id="as-someone-who-successfully-navigated-the-graduate-school-application-process-what-advice-would-you-offer-to-current-iitgn-students-aiming-for-institutions-like-mit"><strong>As someone who successfully navigated the graduate school application process, what advice would you offer to current IITGN students aiming for institutions like MIT?</strong></h4> <p>I’ll speak specifically about PhD applications, as that’s what I’m familiar with - I can’t really comment on Master’s programs as my experience there is limited.</p> <p>The most crucial advice that my mentors gave me when applying was: “Choose an advisor over the school name”. PhD is a long-term commitment and your advisor will have more impact on your research journey than the school. Here’s a personal example - I actually didn’t apply to MIT initially! I chose Cornell because I wanted to work with my current advisor. When she later moved to MIT, I transferred with her.</p> <p>Letters of recommendation are arguably the most crucial part of your application package. To get strong letters, I recommend spending multiple years engaging in research during your undergraduate studies. This serves two important purposes: it helps you understand if you truly enjoy research and what areas interest you most, while also allowing faculty to really get to know your capabilities and write detailed, compelling letters of support.</p> <p>For me, the extended period I spent on research projects during my IITGN years not only helped me discover my interests but also led to strong letters that significantly strengthened my application. Remember, your letter writers need to speak specifically about your research abilities and potential - this kind of detailed insight only comes from sustained research engagement.</p> <h4 id="for-students-looking-to-pursue-a-phd-after-their-bachelors-there-are-typically-two-paths-either-entering-a-phd-program-directly-or-pursuing-an-ms-first-and-then-transitioning-to-a-phd-what-are-your-thoughts-on-these-two-approaches-did-you-face-any-challenges-due-to-the-lack-of-a-masters-degree"><strong>For students looking to pursue a Ph.D. after their bachelor’s, there are typically two paths: either entering a Ph.D. program directly or pursuing an MS first and then transitioning to a Ph.D. What are your thoughts on these two approaches? Did you face any challenges due to the lack of a master’s degree?</strong></h4> <p>I have not faced any challenges in PhD because of not having a Master’s degree. The path you choose depends on your answers to the following two questions - are you confident about pursuing a research career, and is your research profile strong enough for PhD programs? While the second question is relatively straightforward to answer with help from professors, the first requires you to self reflect.</p> <p>If you’re confident about pursuing research and have a solid research profile, going directly into a PhD program makes a lot of sense. It’s the most direct path and saves you valuable time in starting your research career.</p> <p>If you’re committed to research but feel your profile needs strengthening, you have several options. While a master’s degree is one path, I’d particularly recommend exploring research fellowship positions with professors or pre-doctoral programs at institutions like Google Research and Microsoft Research India. These options can be more valuable than a master’s degree since they let you focus purely on research without course requirements.</p> <p>Finally, if you are uncertain about committing to a research career, go for a master’s program with the intention of utilizing any research opportunities that might be available.</p> <h4 id="when-youre-not-immersed-in-research-how-do-you-spend-your-free-time-do-you-have-any-hobbies-or-pastimes-you-enjoy"><strong>When you’re not immersed in research, how do you spend your free time? Do you have any hobbies or pastimes you enjoy?</strong></h4> <p>My relationship with hobbies has changed quite a bit since my IITGN days. During undergrad, I was pretty much a textbook definition of an academic nerd - my closest thing to a hobby was reading books, and my friends would joke that my idea of entertainment was binge-watching research talk videos!</p> <p>Things changed when I started grad school. I realized I needed activities to manage the stress that comes with PhD life, and honestly, the cold American weather kind of forced me to stay active just to keep warm! These days, you’ll find me at the climbing gym or on beginner-level parkour routes. And when I’m not being active, I enjoy more relaxed activities like making chalk art and brewing unusually flavored kombuchas. It’s been a good change - turns out there’s more to life than just research talks!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Answers FAQs about my undergraduate career.]]></summary></entry><entry><title type="html">Landscape of Debuggability in Machine Learning for Systems</title><link href="https://varungohil.github.io/blog/2025/mlsys-debuggability/" rel="alternate" type="text/html" title="Landscape of Debuggability in Machine Learning for Systems"/><published>2025-01-01T18:28:00+00:00</published><updated>2025-01-01T18:28:00+00:00</updated><id>https://varungohil.github.io/blog/2025/mlsys-debuggability</id><content type="html" xml:base="https://varungohil.github.io/blog/2025/mlsys-debuggability/"><![CDATA[<p>As someone interested in computer systems and machine learning, the intersection of the two fields - machine learning for systems has always fascinated me. My experience during internships at Google Cloud and my discussion with friends at other hyperscalar operators has revealed an interesting pattern: while there’s widespread interest in using machine learning to improve systems, very few ML-assisted systems make it to production. This gap between academic promise and practical deployment raises important questions about the challenges we face in implementing ML solutions at scale.</p> <p>Academic literature is filled with proof-of-concepts showing ML-assisted systems outperforming traditional methods and heuristics. However, when it comes to actual deployment, the success stories are surprisingly rare. Through an <a href="https://dl.acm.org/doi/10.1145/3364684">experience paper on ML for systems published by Microsoft</a> and my observations, I’ve found that one of the major challenges in the domain is debuggability - the ability to debug the ML-assisted system when the model misbehaves.</p> <p>Having made this observation, when I started looking into the academic literature on debuggability, I found it to be quite diffuse. It was scattered across several communities like software engineering, networking, databases, programming languages, and verification. This blog is my attempt at connecting the work done by these communities into an intuitive framework that makes it easier for me to understand and navigate the research landscape of debuggability in ML for systems.</p> <h2 id="core-properties">Core Properties</h2> <p>While I’ve talked about debuggability till now, there are three properties at the core of this discussion. Here are the three properties and there definitions :</p> <ul> <li>Debuggability: the ability to detect and resolve issues that lead to model misbehavior.</li> <li>Interpretability: There are two definitions popularized by <a href="https://christophm.github.io/interpretable-ml-book/">Molnar’s book</a>: <ul> <li>Interpretability is the degree to which a human can understand the cause of a decision.</li> <li>Interpretability is the degree to which a human can consistently predict the model’s result.</li> </ul> <p>There is a subtle difference between the two - the second definition does not require a causal understanding of the model’s behavior. As long as we can predict its behavior reliably, we say the model is interpretable. This is akin to saying that the phenomena of sun-rise is interpretable by the explanation that the “Sun rises in the east every day” with no actual causal understanding of why the sun rises in the east every day.</p> <p>The first definition is more strict as it pushes for a causal understanding.</p> </li> <li> <p>Generalizability: model’s ability to perform well on data that is not independent and identically distributed (IID) as the model’s training data.</p> <p>This is an important property as all <a href="https://www.deeplearningbook.org/">ML algorithms rely on the assumption that train and test data are IID</a>. Real world environment frequently <a href="https://ieeexplore.ieee.org/document/10488711">violate this assumption</a> due to user behavior pattern changes, workload churn and hardware heterogeneity, and this results in degradation of model performance.</p> </li> </ul> <p>These properties don’t exist in isolation and relate to each other in ways that help one property improve the other. Consider this: ML models inherently have finite generalizability, which means they can struggle with data that doesn’t match their training distribution. This limitation makes model debuggability essential. When debugging the model, we often need to interpretability to understand why the model made certain decisions to find the root cause of the problem. The insights we gain from this interpretation process then feed back into improving the model’s generalizability.</p> <p>Having defined the core properties, lets move on to the process of debugging. The practice of debugging ML systems involves four essential components:</p> <ul> <li>Detecting model misbehavior</li> <li>Root causing misbehavior by gaining insights</li> <li>Correcting model misbehavior</li> <li>Preventing model misbehavior</li> </ul> <h2 id="detecting-model-misbehavior">Detecting Model Misbehavior</h2> <p>The two interesting questions in this subspace are</p> <ul> <li>How to detect model misbehavior?</li> <li>When to detect model misbehavior?</li> </ul> <p>The answer to the first question depends on the answer to the second question. Hence, I will taxonomize the discussion based on potential answers to the second question.</p> <p>One can detect model misbehavior:</p> <ul> <li>Reactively : once the model is deployed and performed inference to generate prediction</li> <li>Proactively after Deployment : once the model is deployed but before/along with the model’s inference.</li> <li>Proactively before Deployment : before the model is deployed, but after it has been trained and validated.</li> </ul> <h3 id="reactive-misbehavior-detection">Reactive Misbehavior Detection</h3> <p>Here, if we have the ground truth available, we can compare predictions with ground truth - for instance, comparing predicted versus actual latencies in power management systems like <a href="https://ieeexplore.ieee.org/document/9773201">ReTail</a>. In cases where ground truth isn’t readily available, like in autoscaling systems when model output is number of service replicas, we can monitor target metrics such as SLO violations to identify potential issues with model behavior.</p> <h3 id="proactive-misbehavior-detection">Proactive Misbehavior Detection</h3> <p>Proactive detection takes a more preventive approach and can occur at two crucial stages. During deployment we can run uncertainty estimators during/before model’s inference to measure the model’s confidence in its predictions, as shown in Figure below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mlsys-debuggability/uncertainty-aware-systems-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mlsys-debuggability/uncertainty-aware-systems-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mlsys-debuggability/uncertainty-aware-systems-1400.webp"/> <img src="/assets/img/mlsys-debuggability/uncertainty-aware-systems.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Logical block diagram of an uncertainty-aware system <a href="https://ieeexplore.ieee.org/document/10488711">(Source)</a> </div> <p>These estimators can be as simple as distance measurements or can be more complex like density estimation tools or Bayesian neural networks that output probability distributions rather than point estimates. When uncertainty levels are too high, one can choose to ignore the model’s predictions entirely.</p> <p>We can also diagnose model misbehavior before deployment! There are two major approaches proposed here:</p> <ul> <li>Data slicing</li> <li>Model verification</li> </ul> <h4 id="data-slicing">Data Slicing</h4> <p>The goal of data slicing is to identify subsets of data (”slices”) where the model might misbehave. A slice is defined as a conjunction of feature-value pairs and the process of slice identification is run on a held-out validation dataset. A problematic slice is identified based on testing of a significant difference of model performance metrics (e.g.,loss function) of the slice and its counterpart. As datasets have grown larger, this process has been automated. The common approached for slice identification are:</p> <ul> <li><strong>Clustering:</strong> This approach involves grouping similar examples together and treating each cluster as a data slice. If a model underperforms on any of the slices, the user can analyze the examples within that cluster. However, this approach has drawbacks, such as difficulty interpreting high-dimensional data and the need to specify the number of clusters.</li> <li><strong>Decision Tree Training:</strong> This method trains a decision tree to classify which slices are problematic. The leaves of the decision tree correspond to slices. While this offers a natural interpretation, it may not find all problematic slices, as it only searches non-overlapping slices.</li> <li><strong>Lattice Searching:</strong> This approach considers a larger search space where slices can overlap. It traverses a lattice of slices in a breadth-first manner, checking for effect size and statistical significance. This method can be more expensive but provides a more comprehensive search. <em>**</em></li> </ul> <p>Relevant papers: <a href="https://www.computer.org/csdl/journal/tk/2020/12/08713886/1a316PSx4kw">Automated Data Slicing</a>, <a href="https://dl.acm.org/doi/10.1145/3468264.3468614">Explaining Mispredictions </a></p> <h4 id="model-verification">Model Verification</h4> <p>Verification is a way to guarantee that the model meets a user-specified requirement or alternatively, for identifying a scenario or counterexample where the requirement is violated. This generation of counter-examples helps identify and detect model misbehavior before deployment.</p> <p>This definitely necessitates that users are able to formally specify their requirements or properties as <em>specifications</em> understood by the verifier.</p> <p>The most common types of properties are:</p> <ul> <li>safety: guarantees that nothing bad happens</li> <li>liveness: guarantees that something good happens eventually</li> <li>monotonicity: output decreases/increases in line with input in a monotonic fashion</li> </ul> <p>Next, the model and system is encoded in a graph representing states and the corresponding transitions. Once this encoding is done, the specified properties can be checked by verifier using techniques like bounded model checking, k-inductions and invariant inference.</p> <p>The major issue with verification is poor scalability. However, prior works have shown that verification is still useful for models deployed for systems tasks as:</p> <ul> <li>The models used for systems tasks are small</li> <li>The input features are highly engineered which allows expression of complex system properties</li> </ul> <p>For a more detailed discussion on neural network verification for systems, please refer to <a href="#neural-network-for-systems-nn4sysverification">Appendix</a>. Yes, I geeked out and wrote an appendix for a blog post!</p> <h2 id="root-causing-misbehavior-by-gaining-insights">Root causing misbehavior by gaining insights</h2> <p>This is where interpretability of model plays a key role. To reiterate the purpose of interpretability is to gain insight to explain the model’s decision making process. However, it is not clear what is a good explanation and what insights help in explaining the model’s decision making process. I briefly talk about some work on <a href="#notions-of-a-good-explanation">formalizing the notion of a good explanation</a> at the end of this section.</p> <p>The insights one aims to gain from interpreting the model depend on one’s <a href="#core-properties">definition of interpretability.</a> If one follows the second definition of interpretability given previously, then one is only interested in identifying patterns in model’s decisions which make the model’s decision-making predictable. Here, one is only interested in answering the question “When does a ML model misbehave?”. On the other hand if one follows the first definition of interpretability given previously, then one is interested in answering “Why does a ML model misbehave?”.</p> <h3 id="when-does-a-ml-model-misbehave">When does a ML model misbehave?</h3> <p>Here, the idea to discover patterns to find when the model is predictably wrong. This usually translates to identifying data samples on which the model misbehaves. <a href="#data-slicing">Data slicing</a> is the approach used for identifying subsets of data on which the model has low performance. These data subsets or slices are identifies using a decision list - a conjunction of feature-value pairs. These slices are considered interpretable when the length of decision list is small i.e they only have a handful of feature-value pairs.</p> <h3 id="why-does-a-ml-model-misbehave">Why does a ML model misbehave?</h3> <p>The aim of this question is to uncover the causal structure of the model’s decision making process. This is an ambitious goal where people have only seen partial success. Below are some methods that researchers have proposed:</p> <h4 id="building-surrogate-models">Building surrogate models</h4> <p>The idea is to train a inherently interpretable model (surrogate) that mimics the original complex model. Surrogates are trained using some form of teacher-student training.</p> <p>Since the surrogate is inherently interpretable like a decision tree or spare linear model, one can easily extract the model’s decision making process from it. While this seems great , this approach has the underlsing assumption that since the decisions made by the surrogate mimic that of the original model, the decision-making process of the surrogate also mimics that of the original model. This assumption is not necessarily true. Even if that assumption holds, since the surrogates have low complexity, it is usually not possible to capture the entire decision-making process of the complex original model in the surrogate. This results in surrogates having lower performance than the original model and exposes a complexity-performance tradeoff.</p> <p>Relevant Papers: <a href="https://dl.acm.org/doi/10.1145/3387514.3405859">Interpreting DL-based Networking Systems</a>, <a href="https://www.usenix.org/conference/atc22/presentation/hu">PRIMO</a></p> <h4 id="counterfactuals">Counterfactuals</h4> <p>Counterfactuals help us answer “what-if” questions. For example, <em>what</em> would be the predicted latency <em>if</em> the cpu utilization was 2X instead of X? Usually, the explanations here provide data examples where the model’s prediction changes significantly on a minor pertubation to the input or an input feature. Usually counterfactuals are implemented using causal inference where the system is modeled usng <a href="https://www.bradyneal.com/causal-inference-course">Bayesian Networks or Structural Causal Models (SCM)</a>.</p> <p>The disadvantage of this approach is that for <a href="https://christophm.github.io/interpretable-ml-book/counterfactual.html">each input sample one can generate multiple counterfactual explanations</a>. Some of this counterfactual explanations might be inconsistent with each other. Hence, it is difficult to figure out which counterfactual explanations to use.</p> <p>Relevant Papers: <a href="https://dl.acm.org/doi/10.1145/3510457.3513081">Counterfactual Explanations for Models of Code</a></p> <h4 id="feature-attributions">Feature attributions</h4> <p>The idea here is to assign an importance score to each feature in the input or provide a relative ranking of feature importances. The intuition being that the model misbehavior can be explained most by the feature having the largest contribution/importance in producing the model’s output. Multiple techniques like <a href="https://christophm.github.io/interpretable-ml-book/local-methods.html">LIME, Shapely values, SHAP</a> can be used to get this feature importances. For more complex networks like LSTMs or transformers, attention values are used to obtain feature importance. and many time simple techniques can lead to important insights. For example in <a href="https://dl.acm.org/doi/10.1145/3445814.3446693">SINAN</a>, authors were able to identify a faulty configuration of redis using LIME to interpret their CNN model.</p> <p>These approaches provide useful insight into which features are the most relevant and can act as good starting points in the debugging process. However, their disadvantage is that they do not provide any understanding of what the model is learning and how the features interact together to produce the final prediction. Hence, it has minimal contribution to out understanding of the causal structure of the model’s decision making process.</p> <p>Relevant Papers: <a href="https://dl.acm.org/doi/10.1145/3445814.3446693">SINAN</a>, <a href="https://dl.acm.org/doi/10.1145/3352460.3358319">Applying Deep Learning To The Cache Replacement Problem</a></p> <h4 id="data-attributions">Data attributions</h4> <p>The general idea behind data attribution is to identify the training data samples that contributed most to the model misbehavior. The assumption here is that the set of training samples contributing most to model behavior can be easily grouped together to represent a human-understandable concept.</p> <p>It is easy to confuse data attribution with data slicing, however, there is a subtle difference. Data slicing seeks to identify data subsets on which the model misbehaves. On the other hand, data attribution seeks to identify training samples which contribute most towards model (mis)behavior. So for a model behavior, data attribution provides the training examples which contributed the most positively and negatively to the model behavior. The difference is that data attribution tries to distinguishes between the training sample on which the model misbehaves and seeks to give them a contribution score.</p> <p>My friends at MIT organized a great tutorial on Data Attribution at ICML 2024. To learn more, please refer to their <a href="https://ml-data-tutorial.org/">tutorial</a>.</p> <h4 id="neuron-attributions">Neuron attributions</h4> <p>The idea here is attribute the model misbehavior to a given fault neuron or a group of neurons in the model. As we are talking about neurons, this approach is targeted towards neural network interpretation.</p> <p><a href="https://dl.acm.org/doi/10.1145/3236024.3236082">MODE</a> identifies the neurons causing misbehavior using a two-step process - first it identifies the layer and next it identifies the neuron within the layer.</p> <p>For doing so it selects a layer, it freezes the network upto the selected layer and adds a new output later (shown in figure below). Then it trains just the output layer again. It repeats the procedure for all hidden layers in the network. It selects the first hidden layer for which the model’s accuracy it higher or equal to the original model’s accuracy. The intuition is that the features represented by the selected layer are good enough to perform the prediction and later layers are not essential . Finally, to select the neuron within the selected layer, the authors analyze the weights between the selcted layer and the new output layer. Higher weight magniutde signifies higher importance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mlsys-debuggability/state-diff-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mlsys-debuggability/state-diff-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mlsys-debuggability/state-diff-1400.webp"/> <img src="/assets/img/mlsys-debuggability/state-diff.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Overview of frozen model generation for layer and neuron selection <a href="https://dl.acm.org/doi/10.1145/3236024.3236082">(Source)</a> </div> <p>The machine learning community has also directed its efforts towards “<em>mechanistic interpretability</em>” - a field aimed at reverse-engineering the model. Initial efforts have shown that neurons/neuron groups within a model can be mapped to human-understandable concepts and algorithms learned by model to combine the different concepts together to form the output. Future efforts in this direction by systems researchers can help reveal the underlying algorithm learned by the model and help us move forward from current attribution-based appraoches.</p> <h3 id="notions-of-a-good-explanation">Notions of a good explanation</h3> <p>It is surprisingly difficult to define a good explanation.</p> <p>Christopher Molnar says that a good explanation has the following properties - accuracy, fidelity, consistency, stability, comphrehensibility, certainty degree of importance, novelty and representativeness. For definition of the properties refer to the <a href="https://christophm.github.io/interpretable-ml-book/properties.html">Interpretable Machine Learning book.</a></p> <p><a href="https://arxiv.org/pdf/2306.06134">Jia et. al.</a> have formalized the notion of a “sound explanation” by restricting it to four properties- specificity, additivity, completeness, and baseline invariance.</p> <p>Here are the definitions of these properties (copied from the <a href="https://arxiv.org/pdf/2306.06134">paper</a>):</p> <ul> <li>Specificity: An interpretation should be specific to influential inputs: it should assign zero scores to inputs that do not influence function values.</li> <li>Additivity An interpretation is additive if its attribution decomposes over function addition.</li> <li>Completeness An interpretation is complete if its attribution explains all of the output.</li> <li>Baseline invariance A baseline-invariant interpretation generates the same rank of inputs for different choices of baseline input.</li> </ul> <p>While this formalization might seem arbitrary, the paper discusses good logical arguments for including each property. More importantly, it leads to a crucial theorem : No attribution algorithm can satisfy all 4 four of the aforementioned properties at once. This discourages the practice of interpreting models by assigning attribution scores to input components.</p> <h2 id="correcting-model-misbehavior">Correcting model misbehavior</h2> <p>Model’s parameters dictate the model’s behavior. Hence, correcting the model’s behavior translates to modifying the model’s parameters. The widely adopted way of modify model’s parameters is to use a training algorithm like gradient descent. However, if one knows the neuron-level attributions for a neural network one can also manually adjust the parameter of the neuron to change model’s behavior</p> <h3 id="retraining">Retraining</h3> <p>The most obvious way to correct mode behavior is to retrain the model on a dataset which has examples on which the model misbehaves. Machine learning models deployed in production are <a href="https://ieeexplore.ieee.org/document/9407135">usually trained periodically</a> on all collected data to handle issues of distribution drifts (poor generalization).</p> <p>Unfortunately, training is a time-consuming, resource intense and costly process. Hence, any efforts to optimize the training process are useful.</p> <p>There seem to be two questions that are important to answer when optimizing retraining:</p> <ul> <li>When to retrain the model?</li> <li>On what data to retrain the model?</li> </ul> <h4 id="when-to-retrain-the-model">When to retrain the model?</h4> <p>We want to retrain the model only when needed. That is to say, when the we see a distribution shift in the input data (data distribution is OOD) and the benefit of retraining out weights its overhead.</p> <p>It is easy to estimate the overhead of retraining since one knows it from past experience. However, is non-trivial to estimate the benefit of retraining the model. Instead one can approximate benefit of retraining by using the uncertainty of model’s predictions averaged over recent predictions. One can also use Out-Of-Distribution (OOD) detection methods to detect a distribution shift and trigger retraining based on this detector.</p> <p>Relevant paper: <a href="https://arxiv.org/abs/2410.02980#:~:text=We%20propose%20DecTrain%2C%20a%20new,with%20the%20predicted%20accuracy%20gain.">DecTrain</a></p> <h4 id="on-what-data-to-retrain-the-model">On what data to retrain the model?</h4> <p>The retraining dataset should obviously include examples of data on which the model misbehaves. The dataset should also include examples where the model already behaves well. Examples of both types (where model misbehaves and where model behaves well) are essential to avoid dataset imbalance issues.</p> <p>What is not obvious is how many of each type of examples should be in the retraining dataset and which examples of each type should be included.</p> <p>To answer these questions, <a href="https://www.notion.so/The-Landscape-of-Debuggability-in-Machine-Learning-for-Systems-16b370ae59c0800480e3cedbe3cad4bd?pvs=21">data attribution methods</a> can be useful. Data attribution tells us which examples in the training set had the most positive/negative contribution to the model’s behavior. Hence, when creating a dataset for treating model misbehavior one should include:</p> <ul> <li>deployment-time examples on which model misbehaved</li> <li>training set examples which had most negative contribution to model misbehavior.</li> <li>training set examples which had the most positive contribution to desired model behavior.</li> </ul> <p>To decide how many examples to include, once can decide a threshold attribution value and include examples above that threshold.</p> <p>Currently, data attribution methods work well on strongly convex models, however this is a growing field and I hope we will soon have efficient predictive attribution estimators for deep neural networks as well.</p> <h3 id="model-surgery">Model Surgery</h3> <p>If one uses neuron-level attribution methods to interpret the model, one can map a subset of model or a neuron to a human-understandable concept. If this is possible, then one can potentially just change the weight (parameter) associated with that neuron to change model behavior when dealing with the corresponding concept.</p> <p>This has been shown to work well on non-systems based ML models. Famous examples are the <a href="https://arxiv.org/abs/2202.05262">ROME paper</a> and the <a href="https://www.anthropic.com/news/golden-gate-claude">Golden-Gate Claude</a>. The former identifies neurons with an LLM that store the fact that “Colosseum is in ROME” while the latter identifies neurons in Claude corresponding to the Golden Gate Bridge. By modifying the neurons, the researchers show that they can change fact learned by the model - that is the model will now store the fact that “Colosseum is in Paris” or can force to model to focus on the concept even when not required; in case of Golden Gate bridge Claude, the LLM would inevitably try to include something about the golden gate bridge in its responses.</p> <p>While there has not been MLSys work on model surgery I believe that this would be greatly beneficial in the field and eliminate a large portion of debugging overhead.</p> <p>One underlying assumption with this approach is that individual neurons map to single concept. That is changing the neuron value to improve model behavior on one concept does not degrade its behavior on another concept.</p> <h2 id="preventing-model-misbehavior">Preventing model misbehavior</h2> <p>Much like healthcare, prevention is better than cure in MLSys as well.</p> <p>Below are a few ways to prevent model misbehavior:</p> <h3 id="add-guard-rails">Add guard rails</h3> <p>The simples way to prevent known errors is to limit the model output by adding some guard rails. This guard rails could be in form of output clipping or threshold checking. While very simple to implement it woks only for known issues with the model and most likely will fail on unknown issues.</p> <h3 id="use-predictability-of-model-misbehavior">Use predictability of model misbehavior</h3> <p>As <a href="#proactive-misbehavior-detection">discussed previously</a>, there have been proposals to add an uncertainty estimator to systems to estimate the model’s confidence in it prediction. When the uncertainty is high, the system may choose to ignore the model’s prediction and fall back to a well-studies heuristic method. Further, the same strategy can be used when we have identified data subsets on which model misbehaves using data slicing before deployment.</p> <p>In reinforcement learning settings, similar proposals are popular under the name of <a href="https://arxiv.org/abs/2010.03625">“<em>online safety assurance”</em></a>. At a high level these proposals, fall back to a well-studies heuristic policy when the RL system reaches an “unsafe” region during its exploration phase.</p> <h3 id="use-a-model-ensemble">Use a model ensemble</h3> <p>Here the idea is to use an ensemble of “expert” models and dynamically select the best model from the ensemble for prediction for each input sample. Most of the research focus here seems to be on model selection algorithms.</p> <p>The trivial selection algorithm would be to run all models and then report some aggregate metric of all model outputs. However this is resource intensive. Alternative proposals use a neural network to select which model(s) to use for prediction. <a href="https://www.usenix.org/conference/nsdi23/presentation/khani">RECL</a> use a gating network which assigns a score to each model in the ensemble. Models with score higher than a threshold are used for prediction. <a href="https://arxiv.org/abs/2409.19867">Ivy</a> use meta-learning to learn a model selection policy and uses that to select a single model that is used for prediction.</p> <h3 id="change-model-structure">Change model structure</h3> <p>Model structure provides an “inductive bias” which can help the model learn better on particular domains.</p> <p>The most successful examples here encode the causal relationships between the observable and latent variables as a Bayesian Network. The benefit is not only that the model knows the causal relationships but also that now we can learn the distribution of latent confounders.</p> <p>Onelearn the conditional probabilities between variables using vanilla machine learning. Finally, performing causal inference on this new structure allows them to generate accurate counterfactuals.</p> <p>The downside of this approach is that sometimes the causal relationships are not obvious or too difficult to capture. Further, a fundamental research question in causality is how does one know the number of latent confounders? Hence, researchers using this need to guess the number of latent confounders and hope that their guess leads to a decent approximation of the actual latent confounders.</p> <p>Relevant Papers: <a href="https://dl.acm.org/doi/10.1145/3445814.3446700">SAGE</a>, <a href="https://www.usenix.org/conference/nsdi23/presentation/alomar">CausalSim</a></p> <p>A recent work <a href="https://people.csail.mit.edu/arashne/papers/OSprey.pdf">OSPrey</a>, has shown that separating the model into multiple parts - one for learning system context, one for learning workload characteristics and one for cobining the previosu two parts, can be beneficial for generalization.</p> <h3 id="change-learning-algorithm">Change learning algorithm</h3> <p>The reinforcement learning community has designed several learning algorithms, few of which have shown to improve generalization of learned policies. One such algorithm is curriculum learning.</p> <p>In curriculum learning the training environment distribution is changed to gradually increase the difficulty of training environments or decreasing order of rewardingness. The challenge in applying this training algorithm to MLSys is that we do not have a clear way to measuring the difficulty or rewardingness of a training environment.</p> <p><a href="https://dl.acm.org/doi/10.1145/3544216.3544243">GENET</a> proposes gap-to-baseline as a difficulty measure - how much the RL policy’s performance falls behind a traditional rule-based baseline. If environment has large gap-to-baseline, since the baseline already shows how to perform better in the environment, the RL model may learn to “imitate” the baseline’s known rules while training in the same environment, bringing it on par with—if not better than—the baseline. On the flip side, if an environment has a small or even negative gap-to-baseline, chances are that the environment is intrinsically hard (a possible reason why the rule-based baseline performs badly), or the current RL policy already performs well and thus training on it is unlikely to improve performance by a large margin. GENET applies this algorithm to networking problems like adaptive bit rate selection and congestion control.</p> <p>Another proposal is <a href="https://proceedings.mlsys.org/paper_files/paper/2024/hash/f502981cbe221d857ad409450a7917c3-Abstract-Conference.html">FLASH</a>, which learns a app-environment embedding and uses it for downstream tasks like schedule and autoscaling. To adapt to distribution shifts, it uses metalearning to constant modify the app-environment embedding generator. The high-level diagram is below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mlsys-debuggability/flash-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mlsys-debuggability/flash-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mlsys-debuggability/flash-1400.webp"/> <img src="/assets/img/mlsys-debuggability/flash.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Flash's architecture, showing the integration between meta learner and base learner, for both supervised learning (SL) and reinforcement learning (RL). <a href="https://proceedings.mlsys.org/paper_files/paper/2024/hash/f502981cbe221d857ad409450a7917c3-Abstract-Conference.html">(Source)</a> </div> <p>An alternate propsal, <a href="https://proceedings.mlsys.org/paper_files/paper/2023/hash/f025a252767e89e63426e42b4716d311-Abstract-mlsys2023.html">Ourboros</a>, proposes including the verifier in the training pipeline to ensure that the trained networks follow the provided specifications. Figures below show the high-level block diagrams of such verifier-in-the-loop training processes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mlsys-debuggability/train-verify-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mlsys-debuggability/train-verify-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mlsys-debuggability/train-verify-1400.webp"/> <img src="/assets/img/mlsys-debuggability/train-verify.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mlsys-debuggability/ourboros-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mlsys-debuggability/ourboros-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mlsys-debuggability/ourboros-1400.webp"/> <img src="/assets/img/mlsys-debuggability/ourboros.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Logical block diagrams of proposals for verification-aware training <a href="https://proceedings.mlsys.org/paper_files/paper/2023/hash/f025a252767e89e63426e42b4716d311-Abstract-mlsys2023.html">(Source)</a> </div> <h2 id="appendix">Appendix</h2> <h3 id="neural-network-for-systems-nn4sys-verification">Neural Network for Systems (NN4Sys) Verification</h3> <p>Relevant papers and Sources:</p> <ul> <li><a href="https://dl.acm.org/doi/abs/10.1145/3452296.3472936">Verifying learning-augmented systems</a></li> <li><a href="https://ieeexplore.ieee.org/document/9617684">Towards Scalable Verification of Deep Reinforcement Learning</a></li> <li><a href="https://arxiv.org/abs/2409.10897">AutoSpec: Automated Generation of Neural Network Specifications</a></li> <li><a href="https://arxiv.org/abs/2412.03028">Specification Generation for Neural Networks in Systems</a></li> <li><a href="https://openreview.net/forum?id=mhjRudcHcB#discussion">NN4SysBench: Characterizing Neural Network Verification for Computer Systems</a></li> <li><a href="https://proceedings.mlsys.org/paper_files/paper/2023/hash/f025a252767e89e63426e42b4716d311-Abstract-mlsys2023.html">Building Verified Neural Networks for Computer Systems with Ouroboros</a></li> </ul> <p>Neural networks for systems are attractive candidate for verification. These networks have well-engineered features which allows for smaller network sizes and ease in specifying complex relationships between features. These properties help mitigate the scalability bottlenecks of neural network verification.</p> <p>A neural network verification query has 3 parts:</p> <ul> <li>neural network N</li> <li>input property P</li> <li>output property Q</li> </ul> <p>A DNN verifier answers the question,</p> <p>“Does there exist some input x, such that P(x) is True and Q(N(x)) is also True?”</p> <p>N(x) represents the output vector of the neural network</p> <p>Verifier can produce two types of outputs:</p> <ul> <li>UNSAT, meaning no such input exists</li> <li>SAT, with a concrete example satisfying input</li> </ul> <p>The properties to verify - P and Q are also called specifications.</p> <p>There are multiple formal verification techniques to use this verifier for verifying the specification. Prior work on verifying neural networks used for systems tasks has used (bounded) model checking, k-induction and invariant inference.</p> <p>When using model checking, the system needs to be formalized as a state machine - with a initial state, a set of all possible states, a transition function, and some predicates/functions for classifying a state as good or bad.</p> <p>When verifying deep reinforcement learning policies, the states in this state machine correspond to the inputs of the neural network - which could be the environment state or the history of environment states. The transition function is usually over-approximated i.e. for transition function T, if T(x, y) is False then y is not reachable from x, however if T(x, y) is True, then y might not be reachable from x. The reason for over-approximation is to avoid complexity when defining the transition function. Finally, the one can have a single predicate to classify a state as good/bad, or one can have two separate predicates G and B for classifying states as good and bad respectively.</p> <p>Having encoded the system into a state machine, the next step is to come up with properties to verify or specifications. In systems, useful properties/specifications discovered till now, fall into the following categories:</p> <ul> <li>safety: guarantees that nothing bad happens</li> <li>liveness : guarantees that something good happens eventually</li> <li>monotonicity: output decreases/increases in line with input in a monotonic fashion</li> <li>probabilistic: the neural network follows a rule with some given probability</li> </ul> <p>Usually, when defining Q, we define it to be the negation of our desired property. Hence, if the verifier returns UNSAT, then the property holds. However, if the verifier returns SAT, then the property does not hold.</p> <p>Usually, the properties or specifications are manually written. However, it is difficult to write comprehensive set of specifications for large systems. Hence, researchers have also proposed generating the specifications from data - using techniques like grid-search, clustering and decision tree traversals.</p> <p>Finally, since a particular execution of the systems can have a huge number of state transitions, we need to bound the the number of state transitions we consider in a given execution to improve the scalability of the verification approach. Hence, instead of model checking, we actually perform bounded model checking and also specify the number of transitions/steps (<strong><em>k</em></strong>) that will be considered. When using bounded model checking, one first runs the verifier with a smaller value of <strong><em>k</em></strong>, if the verifier returns SAT then you know for sure that the property does not hold. However, if the verifier returns UNSAT, then we cannot surely say that the property holds and need to try higher values of <strong><em>k.</em></strong> Hence, bounded model checking is useful for refuting a property but cannot be used for proving a property.</p> <p>To prove a property, one can use a verification technique called <em>k-induction.</em> The idea here is to find execution sequences with length of k steps, which can start from any state and for which the property is False. If the property is False, we will find a sequence of length k and we should also be able be find a sequence of length k+1 that violate the property. However, if we find a sequence of length k that violates the property but cannot find a sequence of length k+1 , then the violating sequences of length k are not reachable from the initial states. Hence, the property must be True.</p> <p>Hence, if a veriﬁer ﬁnds that a k-induction query is UNSAT, we know that the corresponding property holds. However, if it returns SAT with a counter-example that does not start at an initial state, we cannot conclude whether the property holds, and must increase the value of k.</p> <p>Whirl 2 proposal combines bounded model checking and k-induction as shown in figure below to either prove or refute a property/specification.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mlsys-debuggability/verify-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mlsys-debuggability/verify-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mlsys-debuggability/verify-1400.webp"/> <img src="/assets/img/mlsys-debuggability/verify.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" max-width="50%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Diagram shows how Whirl 2.0 uses both bounded model checking and k-induction in tandem to either prove or refute a property. <a href="(https://ieeexplore.ieee.org/document/9617684)">(Source)</a> </div> <p>Next, I’ll define invariant inference.</p> <p>An invariant is a partition of the state space into two disjoint sets such that the two sets are disconnected. Such invariants are useful when all initial states are in one set and all bad states are in the other set. This invariant would show that none of the bad states are reachable.</p> <p>Finding such invariants turns out to be undecideable in the general settings and hence the prior work proposes using semi-automated invariance inference. They use the observation that the systems of interest tend to behave like Boolean monotonic functions, they are satisfiable when the input range is large, but become unsatisfiable when input range is narrowed. Finding the tipping point in useful to proving properties.</p> <p>These verification approaches do have there fair share of limitations:</p> <ul> <li>Poor scalability with network size (exponential). This is further exacerbated when verifying RL policy networks as one has to duplicate the network multiple times, once for each RL step.</li> <li>Current verifiers work only for feedforward neural networks with piece-wise linear activation functions.</li> <li>Current verifiers do not support monotonic specifications.</li> </ul>]]></content><author><name></name></author><category term="technical"/><category term="machine-learning"/><category term="computer-systems"/><summary type="html"><![CDATA[Connecting diffused research on debuggability of ML-assisted systems into an intuitive framework.]]></summary></entry></feed>